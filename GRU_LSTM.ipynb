{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejrbdx/Advance-Research-Methods-2024/blob/main/GRU_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbOsWDnT9-1p",
        "outputId": "52c4f538-66d5-4540-841e-d25b2e6650b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Advanced Research Methods/In class assignments/Class 3 25'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaNrq7Wt-DdE",
        "outputId": "5cb9425c-a5aa-4087-b8f1-215b1e5b8318"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Advanced Research Methods/In class assignments/Class 3 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l9ALzwAv9rMF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random, math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import fftpack\n",
        "import torch.utils.data as utils\n",
        "import os\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ismE23vl9rMI"
      },
      "outputs": [],
      "source": [
        "# !gdown 'https://drive.google.com/uc?id=1E_qqe7kfvfApM4hCOBMPoXhyEPyrUJkN'\n",
        "# !gdown 'https://drive.google.com/uc?id=1j-3-lHegY--FDHKZvz86HBtilV2dsP-i'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-JYBrKXv9rMJ"
      },
      "outputs": [],
      "source": [
        "# !unzip 'train.zip' -d '.'\n",
        "# !unzip 'test.zip' -d '.'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_ = pd.read_pickle('tps_df.pkl')"
      ],
      "metadata": {
        "id": "cvGy3EbVS9NW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test = 0.2\n",
        "# df_len =df_.shape[0]\n",
        "# df_train, df_test = df_.iloc[0:df_len-int(test*df_len)],df_.iloc[df_len-int(test*df_len):]\n",
        "# print (df_train.shape, df_test.shape)\n",
        "# df_train.to_csv('train.csv',index = False)\n",
        "# df_test.to_csv('test.csv',index = False)\n",
        "# # df_train.shape,df_test.shape\n",
        "# ## split it into training and test. 80%:20%"
      ],
      "metadata": {
        "id": "DgVAD2tBSuUk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yVUiieLg9rMJ"
      },
      "outputs": [],
      "source": [
        "class TrafficForecast(Dataset):\n",
        "    def __init__(self,csv_path, window, horizon):\n",
        "        self.csv_path = csv_path\n",
        "        self.window = window\n",
        "        self.horizon = horizon\n",
        "        self.df = None\n",
        "        self.seg_ids = None\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "\n",
        "        df = pd.read_csv(self.csv_path)\n",
        "        df['time'] = pd.to_datetime(df['time'])\n",
        "        df['unix_timestamp'] = df['time'].astype(int) / 10**9\n",
        "        df['dow'] = df['time'].dt.weekday\n",
        "        df['hour'] = df['time'].dt.hour\n",
        "        df['min'] = df['time'].dt.minute\n",
        "        self.df = df.sample(frac=1).reset_index(drop=True)\n",
        "        # print (self.df.shape)\n",
        "\n",
        "\n",
        "\n",
        "        self.seg_ids = self.df['segmentID'].unique()\n",
        "        self.setup_forecast()\n",
        "\n",
        "    def setup_forecast(self):\n",
        "        for segid in self.seg_ids:\n",
        "            df_seg_id = self.df[self.df['segmentID'] == segid]\n",
        "            df_seg_id = df_seg_id.sort_values(by='time',ascending=True)\n",
        "            df_seg_id = df_seg_id.fillna(method=\"ffill\")\n",
        "            TI_series = df_seg_id['TrafficIndex_GP'].values\n",
        "            h_series = df_seg_id['hour'].values\n",
        "            # print (len(TI_series))\n",
        "            for t in range(0,len(TI_series)-(self.window+self.horizon)):\n",
        "                x = TI_series[t:t+self.window]\n",
        "                h = h_series[t:t+self.window]\n",
        "                y = TI_series[t+self.window:(t+self.window+self.horizon)]\n",
        "\n",
        "                xh = np.dstack([x,h])\n",
        "\n",
        "                self.inputs.append(xh)\n",
        "                self.targets.append(y)\n",
        "        # print (self.inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "      X = torch.tensor(self.inputs[idx],dtype=torch.float32).reshape(self.window,2)\n",
        "      y=torch.tensor(self.targets[idx],dtype=torch.float32)\n",
        "\n",
        "      return {'inputs':X,'outputs':y}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tobj = TrafficForecast('train.csv',36,36)\n",
        "tobj_test = TrafficForecast('test.csv',36,36)"
      ],
      "metadata": {
        "id": "LQsqWTjzZgRA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t66xtsFm9rMK",
        "outputId": "89be070b-6d62-4e12-8651-e0d0a175cbe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "bs = 1024\n",
        "dataloader_train = DataLoader(tobj, batch_size=bs, shuffle=False, num_workers=4,drop_last=True)\n",
        "dataloader_test = DataLoader(tobj_test, batch_size=bs, shuffle=False, num_workers=4,drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2tZTu6c9rMK",
        "outputId": "05840505-fbad-4e5f-a266-8fed74091c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 36, 2]) torch.Size([1024, 36])\n",
            "torch.Size([1024, 36, 2]) torch.Size([1024, 36])\n",
            "torch.Size([1024, 36, 2]) torch.Size([1024, 36])\n",
            "torch.Size([1024, 36, 2]) torch.Size([1024, 36])\n"
          ]
        }
      ],
      "source": [
        "for idx, data in enumerate(dataloader_test):\n",
        "    print (data['inputs'].shape,data['outputs'].shape)\n",
        "    if idx == 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OXyj2t7A9rML"
      },
      "outputs": [],
      "source": [
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yaQx7m5f9rML"
      },
      "outputs": [],
      "source": [
        "from models import MyLSTM, MyGRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "l2uwpOy69rML"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = MyGRU(n_inputs=2, n_hidden=10, n_rnnlayers=2, n_outputs=36)\n",
        "# model.to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRDeD4kq9rMM",
        "outputId": "c1fdf82f-bacb-4ea3-f01e-e84ffe6d8eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 33%|███▎      | 1/3 [02:11<04:23, 131.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Training Loss: 0.0938, Testing Loss: 0.0046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [04:25<02:12, 132.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Training Loss: 0.0536, Testing Loss: 0.0068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [06:41<00:00, 133.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Training Loss: 0.0401, Testing Loss: 0.0079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "mean_loss = []\n",
        "losses = []\n",
        "test_losses = []\n",
        "mean_test_loss = []\n",
        "n_epochs = 3\n",
        "\n",
        "for it in tqdm(range(n_epochs)):\n",
        "  # zero the parameter gradients\n",
        "  for i_batch, sample_batched in enumerate(dataloader_train):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(sample_batched['inputs'])\n",
        "    loss = criterion(outputs, sample_batched['outputs'])\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  for i_test,sample_test in enumerate(dataloader_test):\n",
        "\n",
        "      outputs = model(sample_test['inputs'])\n",
        "      test_loss = criterion(outputs,sample_test['outputs'])\n",
        "      test_losses.append(test_loss.item())\n",
        "\n",
        "  mean_loss.append(np.mean(losses))\n",
        "  mean_test_loss.append(np.mean(test_losses))\n",
        "  if (it+1) % 1 == 0:\n",
        "    print(f'Epoch {it+1}/{n_epochs}, Training Loss: {np.mean(losses):.4f}, Testing Loss: {np.mean(test_losses):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mLABbIZZ9rMM"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'grumodel.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rOrBDeOj9rMN",
        "outputId": "08c86efe-42dd-4db7-87ab-5183d3813145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyGRU(\n",
              "  (relu): ReLU()\n",
              "  (rnn): GRU(2, 10, num_layers=2, batch_first=True)\n",
              "  (fc): Linear(in_features=10, out_features=36, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model_arch = model\n",
        "model_arch.load_state_dict(torch.load('grumodel.pt'))\n",
        "model_arch.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model_arch,dataloader_test, bs):\n",
        "    rmse = []\n",
        "    maes = []\n",
        "\n",
        "    for i_test,sample_test in enumerate(dataloader_test):\n",
        "\n",
        "        preds = model_arch(sample_test['inputs'])\n",
        "        targets = sample_test['outputs']\n",
        "        error = ((preds-targets)**2).sum(1).mean().detach().numpy()\n",
        "        mae = (preds-targets).abs().sum(1).mean().detach().numpy()\n",
        "        rmse.append(math.sqrt(error))\n",
        "        maes.append(mae)\n",
        "    return {'rmse':np.mean(rmse),'mae':np.mean(maes)}"
      ],
      "metadata": {
        "id": "f7dIGEVjobPb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluation(model_arch,dataloader_test,bs)"
      ],
      "metadata": {
        "id": "rIGCYJGVqaOQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njSMo_B3dL6C",
        "outputId": "c48100c1-b918-47a5-862b-ab193a50c26b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rmse': 0.5975147327956473, 'mae': 3.411336}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}